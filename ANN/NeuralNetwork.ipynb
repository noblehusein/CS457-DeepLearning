{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-2*math.pi,2*math.pi,1000)\n",
    "y_actual = np.sin(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialise_Neural_Network(input_size,output_size, num_layers, num_neurons):\n",
    "    Neural_Network = [];\n",
    "    Weight_Matrix = [];\n",
    "    Bias_Values = [];\n",
    "    for i in range(num_layers):\n",
    "        temp = np.ones((num_neurons[i],1));\n",
    "        Neural_Network.append(temp);\n",
    "        if(i!=0):\n",
    "            temp_matrix = np.ones((num_neurons[i],num_neurons[i-1]))\n",
    "            Weight_Matrix.append(temp_matrix);\n",
    "            temp = np.ones((num_neurons[i], 1));\n",
    "            Bias_Values.append(temp);\n",
    "    return Neural_Network, Weight_Matrix, Bias_Values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.]]), array([[1.],\n",
      "       [1.]]), array([[1.],\n",
      "       [1.]]), array([[1.]])]\n",
      " \n",
      " \n",
      "[array([[1.],\n",
      "       [1.]]), array([[1., 1.],\n",
      "       [1., 1.]]), array([[1., 1.]])]\n",
      " \n",
      " \n",
      "[array([[1.],\n",
      "       [1.]]), array([[1.],\n",
      "       [1.]]), array([[1.]])]\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "num_layers = 4;\n",
    "input_size = 1;\n",
    "output_size = 1;\n",
    "num_neurons = [input_size, 2, 2, output_size];\n",
    "Neural_Network, Weight_Matrix, Bias_Values = Initialise_Neural_Network(input_size, output_size, num_layers, num_neurons);\n",
    "print(Neural_Network, end = \"\\n \\n \\n\");\n",
    "print(Weight_Matrix, end = \"\\n \\n \\n\");\n",
    "print(Bias_Values, end = \"\\n \\n \\n\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(z):\n",
    "    return np.tanh(z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function_derivative(z):\n",
    "    return (1 - ((np.tanh(z))**2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_derivative(y):\n",
    "    zL = np.dot(Weight_Matrix[-1], Neural_Network[-2]) + Bias_Values[-1];\n",
    "    aL = activation_function(zL);\n",
    "    cost_derivative = np.multiply((aL - y),(activation_function_derivative(zL)));\n",
    "    return cost_derivative;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_Propagation(x):\n",
    "    Neural_Network[0] = x; \n",
    "    for i in range(num_layers-1):\n",
    "        Neural_Network[i+1] = activation_function(np.dot(Weight_Matrix[i], Neural_Network[i]) + Bias_Values[i]);\n",
    "    return Neural_Network[-1][0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_Propagation(cost_derivative_average, alpha):\n",
    "    bias_derivative = 0;\n",
    "    prev_bias_derivative = 0;\n",
    "    weight_derivative = 0;\n",
    "    prev_weight_derivative = 0;\n",
    "    dczL = cost_derivative_average;\n",
    "    for i in reversed(range(len(Weight_Matrix))):\n",
    "        #print(\"Weight Matrix before \", Weight_Matrix[i].shape);\n",
    "        #print(\"Bias Vector before \", Bias_Values[i].shape);\n",
    "        if(i == len(Weight_Matrix) - 1):\n",
    "            dCdw = dczL * Neural_Network[i].transpose();\n",
    "            Weight_Matrix[i] = Weight_Matrix[i] - alpha*dCdw;\n",
    "            Bias_Values[i] = Bias_Values[i] - dczL;\n",
    "        elif(i!=0):\n",
    "            zl = np.dot(Weight_Matrix[i], Neural_Network[i]) + Bias_Values[i];\n",
    "            dczl = np.multiply((Weight_Matrix[i].transpose() * dczL) , activation_function_derivative(zl));\n",
    "            dCdw = dczl * Neural_Network[i].transpose();\n",
    "            Weight_Matrix[i] = Weight_Matrix[i] - alpha*dCdw;\n",
    "            Bias_Values[i] = Bias_Values[i] - dczl;\n",
    "            dczL = dczl;\n",
    "        #rint(\"Weight Matrix after \", Weight_Matrix[i].shape);\n",
    "        #print(\"Bias Vector after \", Bias_Values[i].shape);\n",
    "        #print(i, \" ----------------------------------------------------\");\n",
    "    return \"work plis\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[783.11236465]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n",
      "[763.23943002 763.23943002]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100;\n",
    "alpha = 0.1;\n",
    "for i in range(epochs):\n",
    "    epoch_cost = 0;\n",
    "    cost_gradient = 0;\n",
    "    cost_derivative_average = 0;\n",
    "    for (training_instance, y) in zip(X, y_actual):\n",
    "        output = Forward_Propagation(training_instance);\n",
    "        epoch_cost = epoch_cost + 0.5*((y - output)**2);\n",
    "        y_pred = Forward_Propagation(np.array([[training_instance]]));\n",
    "        cost_derivative_average = cost_derivative_average + calculate_cost_derivative(y);\n",
    "    print(epoch_cost);\n",
    "    Backward_Propagation(cost_derivative_average, alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41997434]\n",
      " [0.07065082]\n",
      " [0.00986604]]\n"
     ]
    }
   ],
   "source": [
    "            zL = np.dot(Weight_Matrix[i], Neural_Network[i]) + Bias_Values[i];\n",
    "            bias_derivative = -(cost_gradient * activation_function_derivative(zL))*alpha;\n",
    "            Bias_Values[i] = Bias_Values[i] + bias_derivative;\n",
    "            prev_bias_derivative = bias_derivative;\n",
    "            weight_derivative = np.multiply(-(cost_gradient * activation_function_derivative(zL)) * alpha , activation_function(zL));\n",
    "            Weight_Matrix[i] = Weight_Matrix[i] + weight_derivative;\n",
    "            prev_weight_derivative = weight_derivative;\n",
    "            \n",
    "            zL = np.dot(Weight_Matrix[i], Neural_Network[i]) + Bias_Values[i];\n",
    "            bias_derivative = -(Weight_Matrix[i] * prev_bias_derivative * activation_function_derivative(zL)) * alpha;\n",
    "            Bias_Values[i] = Bias_Values[i] + bias_derivative;\n",
    "            prev_bias_derivative = bias_derivative;\n",
    "            weight_derivative = np.multiply(-(Weight_Matrix[i] * prev_weight_derivative * activation_function_derivative(zL)) * alpha , activation_function(zL));\n",
    "            Weight_Matrix[i] = Weight_Matrix[i] + weight_derivative;\n",
    "            prev_weight_derivative = weight_derivative;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
